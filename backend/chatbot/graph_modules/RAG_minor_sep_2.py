from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import pandas as pd
import time
import faiss
import numpy as np
from pathlib import Path
from rank_bm25 import BM25Okapi
import re
from typing import List, Tuple, Dict
from dotenv import load_dotenv
import os

load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

class HybridRetriever:
    def __init__(self, vectorstore, docs, k: int = 3, vector_weight: float = 0.4):
        if not docs:  # ë¬¸ì„œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
            raise ValueError("ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒì˜ ë¬¸ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.vectorstore = vectorstore
        self.k = k
        self.docs = docs
        self.vector_weight = vector_weight
        self.keyword_weight = 1 - vector_weight

        # BM25ë¥¼ ìœ„í•œ í† í°í™”ëœ ë¬¸ì„œ ì¤€ë¹„
        self.tokenized_docs = [self._tokenize(doc.page_content) for doc in docs]
        self.bm25 = BM25Okapi(self.tokenized_docs)

        # ì„œìš¸ì‹œ êµ¬ ë¦¬ìŠ¤íŠ¸
        self.districts = [
            "ì„œìš¸ ì¢…ë¡œêµ¬", "ì„œìš¸ ì¤‘êµ¬", "ì„œìš¸ ìš©ì‚°êµ¬", "ì„œìš¸ ì„±ë™êµ¬", "ì„œìš¸ ê´‘ì§„êµ¬", "ì„œìš¸ ë™ëŒ€ë¬¸êµ¬",
            "ì„œìš¸ ì¤‘ë‘êµ¬", "ì„œìš¸ ì„±ë¶êµ¬", "ì„œìš¸ ê°•ë¶êµ¬", "ì„œìš¸ ë„ë´‰êµ¬", "ì„œìš¸ ë…¸ì›êµ¬", "ì„œìš¸ ì€í‰êµ¬",
            "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬", "ì„œìš¸ ë§ˆí¬êµ¬", "ì„œìš¸ ì–‘ì²œêµ¬", "ì„œìš¸ ê°•ì„œêµ¬", "ì„œìš¸ êµ¬ë¡œêµ¬", "ì„œìš¸ ê¸ˆì²œêµ¬",
            "ì„œìš¸ ì˜ë“±í¬êµ¬", "ì„œìš¸ ë™ì‘êµ¬", "ì„œìš¸ ê´€ì•…êµ¬", "ì„œìš¸ ì„œì´ˆêµ¬", "ì„œìš¸ ê°•ë‚¨êµ¬", "ì„œìš¸ ì†¡íŒŒêµ¬", "ì„œìš¸ ê°•ë™êµ¬"
        ]

        # ì¹´í…Œê³ ë¦¬ ì„¤ì •
        self.categories = {
            "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸"],
            "ë§›ì§‘": ["ë§›ì§‘", "ìŒì‹ì ", "ì‹ë‹¹", "ë ˆìŠ¤í† ë‘", "ë§›ìˆëŠ”"],
            "ê³µì—°": ["ê³µì—°", "ì—°ê·¹", "ë®¤ì§€ì»¬", "ì˜¤í˜ë¼"],
            "ì „ì‹œ": ["ì „ì‹œ", "ì „ì‹œíšŒ", "ê°¤ëŸ¬ë¦¬", "ë¯¸ìˆ ê´€"],
            "ì½˜ì„œíŠ¸": ["ì½˜ì„œíŠ¸", "ê³µì—°ì¥", "ë¼ì´ë¸Œ", "ìŒì•…"]
        }

        # ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ê·¸ë£¹ ì„¤ì •
        self.keyword_groups = {
            "ìˆ¨ì€": ["ìˆ¨ì€", "ìˆ¨ê²¨ì§„", "ì•Œë ¤ì§€ì§€ ì•Šì€", "ë¹„ë°€", "íˆë“ ", "hidden", "secret", "ì˜ ëª¨ë¥´ëŠ”", "ë‚¨ë“¤ì´ ëª¨ë¥´ëŠ”", "ë‚˜ë§Œ ì•„ëŠ”", "ë‚˜ë§Œ ì•Œê³  ìˆëŠ”", "ë¶ë¹„ì§€ ì•ŠëŠ”", "í•œì í•œ"],
            "ìš°ì—°": ["ìš°ì—°íˆ", "ìš°ì—°í•œ", "ìš°ì—°íˆ ë°œê²¬í•œ", "ìš°ì—°íˆ ì•Œê²Œ ëœ", "ìš°ì—°íˆ ì°¾ì€", "ìš°ì—°íˆ ë°©ë¬¸í•œ", "ìš°ì—°íˆ ê°€ê²Œ ëœ"],
            "ë¡œì»¬": ["ë¡œì»¬", "í˜„ì§€ì¸", "ì£¼ë¯¼", "ë™ë„¤", "ë‹¨ê³¨", "local", "ê·¼ì²˜", "ì£¼ë³€"]
        }

    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜"""
        return re.findall(r"[\w\dê°€-í£]+", text.lower())

    def _extract_district(self, query: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ ì„œìš¸ì‹œ êµ¬ ì´ë¦„ì„ ì¶”ì¶œ"""
        for district in self.districts:
            if district in query:
                return district

        for district in self.districts:
            district_name = district.replace("ì„œìš¸ ", "")
            if district_name in query:
                return district
        return None

    def _extract_category(self, query: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        query = query.lower()
        for category, keywords in self.categories.items():
            if any(keyword in query for keyword in keywords):
                return category
        return None

    def _check_minor_keywords_in_doc(self, doc_content: str) -> float:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        doc_content = doc_content.lower()
        
        for keyword_type, keywords in self.keyword_groups.items():
            if any(keyword in doc_content for keyword in keywords):
                if keyword_type in ["ìˆ¨ì€", "ìš°ì—°", "ë¡œì»¬"]:  # ì„¸ ê°€ì§€ í‚¤ì›Œë“œ ê·¸ë£¹ ëª¨ë‘ ë†’ì€ ê°€ì¤‘ì¹˜
                    score += 0.4  # ê° ê·¸ë£¹ë‹¹ 0.4ì˜ ê°€ì¤‘ì¹˜
        
        return min(score, 1.0)  # ìµœëŒ€ ì ìˆ˜ëŠ” 1.0

    def _calculate_keyword_scores(self, query: str, filtered_docs: List[Document]) -> List[float]:
        """BM25 ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        tokenized_query = self._tokenize(query)
        filtered_tokenized_docs = [
            self._tokenize(doc.page_content) for doc in filtered_docs
        ]
        filtered_bm25 = BM25Okapi(filtered_tokenized_docs)

        # ê¸°ë³¸ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        base_scores = filtered_bm25.get_scores(tokenized_query)
        
        # ë¬¸ì„œ ë‚´ìš©ì˜ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        minor_scores = [
            self._check_minor_keywords_in_doc(doc.page_content)
            for doc in filtered_docs
        ]
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (BM25 ì ìˆ˜ * 0.5 + ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì ìˆ˜ * 0.5)
        # ë§ˆì´ë„ˆ í‚¤ì›Œë“œì˜ ì¤‘ìš”ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¹„ìœ¨ ì¡°ì •
        final_scores = [
            base_score * 0.5 + minor_score * 0.5
            for base_score, minor_score in zip(base_scores, minor_scores)
        ]

        # ì ìˆ˜ ì •ê·œí™”
        max_score = max(final_scores) + 1e-6
        normalized_scores = [score / max_score for score in final_scores]

        return normalized_scores

    def _calculate_vector_scores(self, query: str, docs: List[Document]) -> List[float]:
        """ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°"""
        query_embedding = np.array(self.vectorstore.embedding_function.embed_query(query)).reshape(1, -1)
        D, indices = self.vectorstore.index.search(query_embedding, min(len(docs), 500))

        vector_scores = [1 - (d / (np.max(D[0]) + 1e-6)) for d in D[0]]  # ê±°ë¦¬ ê¸°ë°˜ ì •ê·œí™”
        return vector_scores

    def _extract_minor_type(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ë§ˆì´ë„ˆ ì¶”ì²œ íƒ€ì… ì¶”ì¶œ"""
        query = query.lower()
        minor_types = []
        
        for minor_type, keywords in self.keyword_groups.items():
            if any(keyword in query for keyword in keywords):
                minor_types.append(minor_type)
        
        return minor_types

    def get_relevant_documents(self, query: str) -> List[Document]:
        """ìµœì¢…ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ê¹Œì§€ ë°˜ì˜í•˜ì—¬ ì •ë ¬"""
        search_start = time.time()
        print("\n=== ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
        print(f"ì…ë ¥ ì¿¼ë¦¬: {query}")

        # 1. ì¿¼ë¦¬ì—ì„œ êµ¬ ì´ë¦„ê³¼ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        district = self._extract_district(query)
        category = self._extract_category(query)
        minor_types = self._extract_minor_type(query)

        if district:
            print(f"\n1. êµ¬ ì´ë¦„ ì¶”ì¶œ: '{district}' ë°œê²¬")
            if category:
                print(f"2. ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ: '{category}' ë°œê²¬")
            if minor_types and category not in ["ì „ì‹œ", "ê³µì—°", "ì½˜ì„œíŠ¸"]:  # ì´ë²¤íŠ¸ê°€ ì•„ë‹ ë•Œë§Œ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¶œë ¥
                print(f"3. ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¶”ì¶œ: {', '.join(minor_types)} ë°œê²¬")
            district_name = district.replace("ì„œìš¸ ", "")

            # 3. í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
            keyword_start = time.time()
            print("\n4. í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ ì¤‘...")

            # 1ë‹¨ê³„: êµ¬ ì´ë¦„ìœ¼ë¡œ í•„í„°ë§
            district_filtered_docs = []
            for doc in self.docs:
                content = doc.page_content.lower()
                location = doc.metadata.get("location", "").lower()
                if (district.lower() in content or district_name.lower() in content or
                    district.lower() in location or district_name.lower() in location):
                    district_filtered_docs.append(doc)
            
            print(f"   - êµ¬ ì´ë¦„ìœ¼ë¡œ í•„í„°ë§ëœ ë¬¸ì„œ ìˆ˜: {len(district_filtered_docs)}ê°œ")

            if len(district_filtered_docs) == 0:
                print("   - êµ¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                return self.vectorstore.similarity_search(query, k=self.k)

            # 2ë‹¨ê³„: ë§ˆì´ë„ˆ í‚¤ì›Œë“œë¡œ í•„í„°ë§ (ì´ë²¤íŠ¸ê°€ ì•„ë‹ ë•Œë§Œ)
            filtered_docs = district_filtered_docs
            if category not in ["ì „ì‹œ", "ê³µì—°", "ì½˜ì„œíŠ¸"]:
                filtered_docs = []
                for doc in district_filtered_docs:
                    content = doc.page_content.lower()
                    has_minor_keyword = False
                    found_keywords = []
                    for keyword_type, keywords in self.keyword_groups.items():
                        if any(keyword in content for keyword in keywords):
                            has_minor_keyword = True
                            found_keywords.append(keyword_type)
                    if has_minor_keyword:
                        filtered_docs.append(doc)
                
                print(f"   - ë§ˆì´ë„ˆ í‚¤ì›Œë“œë¡œ í•„í„°ë§ í›„ ë¬¸ì„œ ìˆ˜: {len(filtered_docs)}ê°œ")
                if len(filtered_docs) == 0:
                    print("   - ë§ˆì´ë„ˆ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    filtered_docs = district_filtered_docs  # ë§ˆì´ë„ˆ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ êµ¬ ê¸°ë°˜ í•„í„°ë§ ê²°ê³¼ ì‚¬ìš©

            # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            scoring_start = time.time()
            print("\n5. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ì¤‘...")

            # ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
            vector_scores = self._calculate_vector_scores(query, filtered_docs)

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            print("   - í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            keyword_scores = self._calculate_keyword_scores(query, filtered_docs)

            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            final_scores = [
                (doc, self.vector_weight * vs + self.keyword_weight * ks)
                for doc, vs, ks in zip(filtered_docs, vector_scores, keyword_scores)
            ]

            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            final_scores.sort(key=lambda x: x[1], reverse=True)
            top_results = [doc for doc, _ in final_scores[: self.k]]

            # ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¶œë ¥ (ì´ë²¤íŠ¸ê°€ ì•„ë‹ ë•Œë§Œ)
            if category not in ["ì „ì‹œ", "ê³µì—°", "ì½˜ì„œíŠ¸"]:
                print("\n=== ì„ íƒëœ ë¬¸ì„œì˜ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ===")
                for i, doc in enumerate(top_results, 1):
                    keywords_found = []
                    doc_content = doc.page_content.lower()
                    print(f"\në¬¸ì„œ {i} ë¶„ì„:")
                    for group, keywords in self.keyword_groups.items():
                        found_keywords = [kw for kw in keywords if kw in doc_content]
                        if found_keywords:
                            keywords_found.append(group)
                            print(f"   - {group} í‚¤ì›Œë“œ ë°œê²¬: {', '.join(found_keywords)}")
                    if keywords_found:
                        print(f"   => ìµœì¢… ë°œê²¬ëœ í‚¤ì›Œë“œ ìœ í˜•: {', '.join(keywords_found)}")
                        # ë§ˆì´ë„ˆ í‚¤ì›Œë“œê°€ ì¿¼ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ê°•ì¡°
                        matching_types = set(keywords_found) & set(minor_types)
                        if matching_types:
                            print(f"   â­ ì¿¼ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ: {', '.join(matching_types)}")
                    else:
                        print("   => ë§ˆì´ë„ˆ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            scoring_time = time.time() - scoring_start
            total_time = time.time() - search_start

            print(f"\n=== ê²€ìƒ‰ ì‹œê°„ ë¶„ì„ ===")
            print(f"í‚¤ì›Œë“œ í•„í„°ë§ ì‹œê°„: {keyword_start - search_start:.2f}ì´ˆ")
            print(f"í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ì‹œê°„: {scoring_time:.2f}ì´ˆ")
            print(f"ì „ì²´ ê²€ìƒ‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"   - ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(top_results)}ê°œ ë¬¸ì„œ")

            return top_results

        print("   - êµ¬ ì´ë¦„ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return self.vectorstore.similarity_search(query, k=self.k)


def check_query_type(query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    event_keywords = {
        "ì „ì‹œ": ["ì „ì‹œ", "ì „ì‹œíšŒ", "ê°¤ëŸ¬ë¦¬", "ë¯¸ìˆ ê´€"],
        "ê³µì—°": ["ê³µì—°", "ì—°ê·¹", "ë®¤ì§€ì»¬", "ì˜¤í˜ë¼"],
        "ì½˜ì„œíŠ¸": ["ì½˜ì„œíŠ¸", "ë¼ì´ë¸Œ", "ê³µì—°ì¥"]
    }
    
    query_lower = query.lower()
    for category, keywords in event_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return "event"
            
    general_keywords = {
        "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸"],
        "ë§›ì§‘": ["ë§›ì§‘", "ìŒì‹ì ", "ì‹ë‹¹", "ë ˆìŠ¤í† ë‘", "ë§›ìˆëŠ”"]
    }
    
    for category, keywords in general_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return "general"
            
    return "general"  # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ê²€ìƒ‰

def setup_rag(query: str):
    # ì¿¼ë¦¬ íƒ€ì… í™•ì¸
    query_type = check_query_type(query)
    print(f"\nì¿¼ë¦¬ íƒ€ì… í™•ì¸: {query_type}")

    # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì–»ê¸°
    current_dir = Path(__file__).resolve().parent.parent.parent  # RA6_vacAItion-dev ë””ë ‰í† ë¦¬ê¹Œì§€
    db_path = current_dir / "data/db/documents.csv"  # .pkl íŒŒì¼ë¡œ ìˆ˜ì •
    event_path = current_dir / "data/event_db/event_data.csv"
    vectorstore_path = current_dir / "data/db/vectorstore"
    event_vectorstore_path = current_dir / "data/event_db/vectorstore"

    print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬: {current_dir}")
    print(f"ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ: {db_path}")
    print(f"ì´ë²¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {event_path}")

    # 1. ë°ì´í„° ë¡œë“œ
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

    # ë³€ìˆ˜ ì´ˆê¸°í™”
    docs = []
    event_docs = []
    vectorstore = None
    event_vectorstore = None
    
    if query_type == "event":
        print("\nì´ë²¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        # ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        if not event_path.exists():
            raise FileNotFoundError(f"ì´ë²¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {event_path}")

        # ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ - ì»¬ëŸ¼ ì´ë¦„ì˜ ê³µë°± ì œê±°
        event_df = pd.read_csv(event_path)
        event_df.columns = event_df.columns.str.strip()
        print(f"ì´ë²¤íŠ¸ CSV íŒŒì¼ì—ì„œ {len(event_df)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        # ì´ë²¤íŠ¸ ë¬¸ì„œ ë³€í™˜
        for idx, row in event_df.iterrows():
            try:
                # ì´ë²¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬
                address_full = f"{row['location']} {row['address']} {row['address_detail']}"
                page_content = f"{row['title']}\nìœ„ì¹˜: {address_full}\nì‹œê°„: {row['time']}\në‚´ìš©: {row['content']}\në¶„ìœ„ê¸°: {row['atmosphere']}\nì¶”ì²œ ë™ë°˜ì: {row['companions']}"
                
                doc = Document(
                    page_content=page_content,
                    metadata={
                        "title": row['title'],
                        "url": "None",
                        "date": row['time'],
                        "location": row['location'],
                        "address": row['address'],
                        "address_detail": row['address_detail'],
                        "type": "event",
                        "tag": row['tag']
                    },
                )
                event_docs.append(doc)
            except Exception as e:
                print(f"ì´ë²¤íŠ¸ ë¬¸ì„œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                continue

        print(f"ë³€í™˜ëœ ì´ë²¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(event_docs)}")

        # ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            event_vectorstore = FAISS.load_local(
                str(event_vectorstore_path), embeddings, allow_dangerous_deserialization=True
            )
            print("ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            if event_docs:
                print("ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                event_vectorstore = FAISS.from_documents(event_docs, embeddings)
                event_vectorstore.save_local(str(event_vectorstore_path))
                print("ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            else:
                raise ValueError("ì´ë²¤íŠ¸ ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\nì¼ë°˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        # ì¼ë°˜ ë°ì´í„° ë¡œë“œ
        if not db_path.exists():
            raise FileNotFoundError(f"ì¼ë°˜ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")

        df = pd.read_csv(db_path)
        print(f"ì¼ë°˜ CSV íŒŒì¼ì—ì„œ {len(df)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        # ì¼ë°˜ ë¬¸ì„œ ë³€í™˜
        for _, row in df.iterrows():
            try:
                content = row["content"]
                if isinstance(content, str):
                    try:
                        metadata_str = content.replace("('metadata', ", "").strip(")")
                        metadata_dict = eval(metadata_str)
                        line_number = metadata_dict.get("line_number", 0)

                        url = row["url"]
                        if isinstance(url, str) and "('page_content', '" in url:
                            content_parts = url.split("('page_content', '")[1].rsplit(
                                "')", 1
                            )
                            if content_parts:
                                page_content = content_parts[0]

                                doc = Document(
                                    page_content=page_content,
                                    metadata={
                                        "title": "None",
                                        "url": url.split("\\t")[0] if "\\t" in url else url,
                                        "date": row["date"],
                                        "line_number": line_number,
                                        "type": "general"
                                    },
                                )
                                docs.append(doc)
                    except Exception as e:
                        continue
            except Exception as e:
                continue

        # ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            vectorstore = FAISS.load_local(
                str(vectorstore_path), embeddings, allow_dangerous_deserialization=True
            )
            print("ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            if docs:
                print("ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                vectorstore = FAISS.from_documents(docs, embeddings)
                vectorstore.save_local(str(vectorstore_path))
                print("ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            else:
                raise ValueError("ì¼ë°˜ ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2. Retriever ì„¤ì •
    if query_type == "event":
        retriever = HybridRetriever(event_vectorstore, event_docs, k=3, vector_weight=0.6)
    else:
        retriever = HybridRetriever(vectorstore, docs, k=3, vector_weight=0.6)

    # 3. LLM ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

    # 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    event_prompt = ChatPromptTemplate.from_template(
        """
    ë‹¤ìŒì€ ê²€ìƒ‰ëœ ì´ë²¤íŠ¸ ì •ë³´ì…ë‹ˆë‹¤:
    {context}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì´ë²¤íŠ¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ê° ì´ë²¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:

    ğŸ’¡ ì¢…í•© ì¶”ì²œ ì˜ê²¬
    [ì „ì²´ì ì¸ ì¶”ì²œ ì´ë²¤íŠ¸ì˜ íŠ¹ì§•ì„ ì„¤ëª…í•˜ê³ , ì§ˆë¬¸ìì˜ ëª©ì ì— ê°€ì¥ ì í•©í•œ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.]

    ===== ì¶”ì²œ ì´ë²¤íŠ¸ ëª©ë¡ =====

    [ë°œê²¬ëœ ê° ì´ë²¤íŠ¸ì— ëŒ€í•´]
    ğŸ¯ [ì´ë²¤íŠ¸ëª…]
    ğŸ“ ìœ„ì¹˜: [ì •í™•í•œ ì£¼ì†Œ]
    â° ê¸°ê°„: [ì§„í–‰ ê¸°ê°„]
    ğŸ·ï¸ ì£¼ìš” íŠ¹ì§•:
    - [íŠ¹ì§• 1]
    - [íŠ¹ì§• 2]
    - [íŠ¹ì§• 3]
    ğŸ’« ì¶”ì²œ ì´ìœ : [ì´ ì´ë²¤íŠ¸ê°€ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
    ğŸ­ ë¶„ìœ„ê¸°: [ì´ë²¤íŠ¸ì˜ ë¶„ìœ„ê¸°]
    ğŸ‘¥ ì¶”ì²œ ê´€ëŒê°: [ëˆ„êµ¬ì™€ í•¨ê»˜ ê°€ë©´ ì¢‹ì„ì§€]

    ì§ˆë¬¸: {question}
    """
    )

    general_prompt = ChatPromptTemplate.from_template(
        """
    ë‹¤ìŒì€ ê²€ìƒ‰ëœ ì¥ì†Œë“¤ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:
    {context}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì¥ì†Œ 3ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ê° ì¥ì†Œì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:

    ğŸ’¡ ì¢…í•© ì¶”ì²œ ì˜ê²¬
    [ì „ì²´ì ì¸ ì¶”ì²œ ì¥ì†Œë“¤ì˜ íŠ¹ì§•ì„ ë¹„êµí•˜ë©°, ì§ˆë¬¸ìì˜ ëª©ì ì— ê°€ì¥ ì í•©í•œ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.]

    ===== ì¶”ì²œ ì¥ì†Œ ëª©ë¡ =====

    1ï¸âƒ£ [ì¥ì†Œëª…]
    ğŸ“ ìœ„ì¹˜: [ì •í™•í•œ ì£¼ì†Œ]
    ğŸ·ï¸ ì£¼ìš” íŠ¹ì§•:
    - [íŠ¹ì§• 1]
    - [íŠ¹ì§• 2]
    - [íŠ¹ì§• 3]
    ğŸ’« ì¶”ì²œ ì´ìœ : [ì´ ì¥ì†Œê°€ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
    ğŸ” ì°¸ê³  ë§í¬: [URL]

    2ï¸âƒ£ [ì¥ì†Œëª…]
    ğŸ“ ìœ„ì¹˜: [ì •í™•í•œ ì£¼ì†Œ]
    ğŸ·ï¸ ì£¼ìš” íŠ¹ì§•:
    - [íŠ¹ì§• 1]
    - [íŠ¹ì§• 2]
    - [íŠ¹ì§• 3]
    ğŸ’« ì¶”ì²œ ì´ìœ : [ì´ ì¥ì†Œê°€ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
    ğŸ” ì°¸ê³  ë§í¬: [URL]

    3ï¸âƒ£ [ì¥ì†Œëª…]
    ğŸ“ ìœ„ì¹˜: [ì •í™•í•œ ì£¼ì†Œ]
    ğŸ·ï¸ ì£¼ìš” íŠ¹ì§•:
    - [íŠ¹ì§• 1]
    - [íŠ¹ì§• 2]
    - [íŠ¹ì§• 3]
    ğŸ’« ì¶”ì²œ ì´ìœ : [ì´ ì¥ì†Œê°€ ì§ˆë¬¸ìì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
    ğŸ” ì°¸ê³  ë§í¬: [URL]

    âœ¨ ì¶”ê°€ íŒ: [ë°©ë¬¸ ì‹œ ì•Œì•„ë‘ë©´ ì¢‹ì„ ì •ë³´ë‚˜ ê¿€íŒì„ ê³µìœ í•´ì£¼ì„¸ìš”]  

    ì§ˆë¬¸: {question}
    """
    )

    # 5. Chain êµ¬ì„±
    def format_docs(docs):
        formatted_docs = []
        for doc in docs:
            content = doc.page_content
            url = doc.metadata.get("url", "None")
            formatted_docs.append(f"{content}\nURL: {url}")
        return "\n\n".join(formatted_docs)

    global last_retrieved_docs
    last_retrieved_docs = []

    def retrieve_and_store(query):
        global last_retrieved_docs
        docs = retriever.get_relevant_documents(query)
        last_retrieved_docs = docs
        return {"docs": docs, "is_event": query_type == "event"}

    def format_and_select_prompt(retrieved_data, question):
        docs = retrieved_data["docs"]
        is_event = retrieved_data["is_event"]
        
        formatted_docs = []
        for doc in docs:
            content = doc.page_content
            url = doc.metadata.get("url", "None")
            formatted_docs.append(f"{content}\nURL: {url}")
        context = "\n\n".join(formatted_docs)
        
        if is_event:
            return event_prompt.format(context=context, question=question)
        else:
            return general_prompt.format(context=context, question=question)

    def stream_handler(chunk):
        print(chunk.content, end="", flush=True)

    # rag_chain ìˆ˜ì •
    retriever_chain = RunnablePassthrough() | (lambda x: {"retrieved_data": retrieve_and_store(x), "question": x})
    prompt_chain = RunnablePassthrough() | (lambda x: format_and_select_prompt(x["retrieved_data"], x["question"]))
    rag_chain = retriever_chain | prompt_chain | llm

    return rag_chain, stream_handler


def print_retrieved_docs():
    print("\n=== ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ===")
    for i, doc in enumerate(last_retrieved_docs, 1):
        print(f"\në¬¸ì„œ {i}:")
        print(f"ë‚´ìš©: {doc.page_content}")
        print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")


if __name__ == "__main__":
    # ì±—ë´‡ ì‹¤í–‰
    query = "ì„œìš¸ ì¢…ë¡œêµ¬ ì¹œêµ¬ë“¤ê³¼ ë³´ê¸° ì¢‹ì€ ì „ì‹œíšŒ ì¶”ì²œ"
    # query = "ì„œìš¸ ì¢…ë¡œêµ¬ ì¹œêµ¬ë“¤ê³¼ ê°€ê¸° ì¢‹ì€ ì¹´í˜ ì¶”ì²œ"
    
    # RAG ì±—ë´‡ ì„¤ì •
    setup_start = time.time()
    rag_chain, stream_handler = setup_rag(query)  # ì¿¼ë¦¬ë¥¼ ì¸ìë¡œ ì „ë‹¬
    setup_time = time.time() - setup_start
    print(f"ì±—ë´‡ ì„¤ì • ì‹œê°„: {setup_time:.2f}ì´ˆ")

    total_start = time.time()

    # ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    print("\n=== AI ë‹µë³€ ===")
    for chunk in rag_chain.stream(query):
        stream_handler(chunk)

    # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print_retrieved_docs()

    # ì‹œê°„ ë¶„ì„ ì¶œë ¥
    total_time = time.time() - total_start
    print(f"\n=== ì „ì²´ ì‹¤í–‰ ì‹œê°„ ë¶„ì„ ===")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ì‘ë‹µ ìƒì„± ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ")