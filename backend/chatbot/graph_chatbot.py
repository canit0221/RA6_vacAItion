import os
import threading
import asyncio
from typing import Dict, List, Annotated, TypedDict, Tuple, Optional, Any, Iterator
from threading import Event
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import pandas as pd
import re
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
import langgraph
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages, MessageGraph
import random
from .graph_modules import (
    GraphState, 
    query_analyzer, 
    hybrid_retriever, 
    naver_search, 
    response_generator
)

# ì‹±ê¸€í†¤ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ì™€ ì´ˆê¸°í™” ìƒíƒœ
_graph_instance = None
graph_ready = Event()
_initialization_in_progress = False  # ì´ˆê¸°í™”ê°€ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ëŠ” í”Œë˜ê·¸

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
if openai_api_key:
    os.environ["OPENAI_API_KEY"] = openai_api_key
else:
    print("Warning: OPENAI_API_KEY environment variable is not set")


def get_graph_instance():
    """ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ - ì‹±ê¸€í†¤ íŒ¨í„´"""
    global _graph_instance
    
    # ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ê³  ì´ˆê¸°í™”ê°€ ì§„í–‰ ì¤‘ì´ì§€ ì•Šìœ¼ë©´ ì´ˆê¸°í™” ì‹œì‘
    if _graph_instance is None and not _initialization_in_progress:
        threading.Thread(target=initialize_graph_in_background, daemon=True).start()
        
    return _graph_instance


# í† í°í™” í•¨ìˆ˜
def tokenize(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜"""
    return re.findall(r"[\w\dê°€-í£]+", text.lower())


# ì¹´í…Œê³ ë¦¬ ë° êµ¬ ì´ë¦„ ì¶”ì¶œ í•¨ìˆ˜
def extract_categories_and_districts(query: str) -> Tuple[Optional[str], Optional[str]]:
    """ì¿¼ë¦¬ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ êµ¬ ì´ë¦„ ì¶”ì¶œ"""
    districts = [
        "ì„œìš¸ ì¢…ë¡œêµ¬", "ì„œìš¸ ì¤‘êµ¬", "ì„œìš¸ ìš©ì‚°êµ¬", "ì„œìš¸ ì„±ë™êµ¬", "ì„œìš¸ ê´‘ì§„êµ¬", "ì„œìš¸ ë™ëŒ€ë¬¸êµ¬",
        "ì„œìš¸ ì¤‘ë‘êµ¬", "ì„œìš¸ ì„±ë¶êµ¬", "ì„œìš¸ ê°•ë¶êµ¬", "ì„œìš¸ ë„ë´‰êµ¬", "ì„œìš¸ ë…¸ì›êµ¬", "ì„œìš¸ ì€í‰êµ¬",
        "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬", "ì„œìš¸ ë§ˆí¬êµ¬", "ì„œìš¸ ì–‘ì²œêµ¬", "ì„œìš¸ ê°•ì„œêµ¬", "ì„œìš¸ êµ¬ë¡œêµ¬", "ì„œìš¸ ê¸ˆì²œêµ¬",
        "ì„œìš¸ ì˜ë“±í¬êµ¬", "ì„œìš¸ ë™ì‘êµ¬", "ì„œìš¸ ê´€ì•…êµ¬", "ì„œìš¸ ì„œì´ˆêµ¬", "ì„œìš¸ ê°•ë‚¨êµ¬", "ì„œìš¸ ì†¡íŒŒêµ¬", "ì„œìš¸ ê°•ë™êµ¬"
    ]
    
    categories = {
        "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸"],
        "ë§›ì§‘": ["ë§›ì§‘", "ìŒì‹ì ", "ì‹ë‹¹", "ë ˆìŠ¤í† ë‘", "ë§›ìˆëŠ”"],
        "ê³µì—°": ["ê³µì—°", "ì—°ê·¹", "ë®¤ì§€ì»¬", "ì˜¤í˜ë¼"],
        "ì „ì‹œ": ["ì „ì‹œ", "ì „ì‹œíšŒ", "ê°¤ëŸ¬ë¦¬", "ë¯¸ìˆ ê´€"],
        "ì½˜ì„œíŠ¸": ["ì½˜ì„œíŠ¸", "ê³µì—°ì¥", "ë¼ì´ë¸Œ", "ìŒì•…"]
    }
    
    # êµ¬ ì´ë¦„ ì¶”ì¶œ
    district = None
    for d in districts:
        if d in query:
            district = d
            break
    
    if not district:
        for d in districts:
            district_name = d.replace("ì„œìš¸ ", "")
            if district_name in query:
                district = d
                break
    
    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    category = None
    query_lower = query.lower()
    for cat, keywords in categories.items():
        if any(keyword in query_lower for keyword in keywords):
            category = cat
            break
    
    return category, district


# ì´ë²¤íŠ¸ ê²€ì‚¬ í•¨ìˆ˜
def check_query_type(query: str) -> str:
    """ì¿¼ë¦¬ íƒ€ì…ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    event_keywords = {
        "ì „ì‹œ": ["ì „ì‹œ", "ì „ì‹œíšŒ", "ê°¤ëŸ¬ë¦¬", "ë¯¸ìˆ ê´€"],
        "ê³µì—°": ["ê³µì—°", "ì—°ê·¹", "ë®¤ì§€ì»¬", "ì˜¤í˜ë¼"],
        "ì½˜ì„œíŠ¸": ["ì½˜ì„œíŠ¸", "ë¼ì´ë¸Œ", "ê³µì—°ì¥"]
    }
    
    query_lower = query.lower()
    for category, keywords in event_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return "event"
            
    general_keywords = {
        "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸"],
        "ë§›ì§‘": ["ë§›ì§‘", "ìŒì‹ì ", "ì‹ë‹¹", "ë ˆìŠ¤í† ë‘", "ë§›ìˆëŠ”"]
    }
    
    for category, keywords in general_keywords.items():
        if any(keyword in query_lower for keyword in keywords):
            return "general"
            
    return "general"  # ê¸°ë³¸ê°’ì€ ì¼ë°˜ ê²€ìƒ‰


# ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_minor_keywords(query: str) -> List[str]:
    """ì¿¼ë¦¬ì—ì„œ ë§ˆì´ë„ˆ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keyword_groups = {
        "ìˆ¨ì€": ["ìˆ¨ì€", "ìˆ¨ê²¨ì§„", "ì•Œë ¤ì§€ì§€ ì•Šì€", "ë¹„ë°€", "íˆë“ ", "hidden", "secret", "ì˜ ëª¨ë¥´ëŠ”", "ë‚¨ë“¤ì´ ëª¨ë¥´ëŠ”", "ë‚˜ë§Œ ì•„ëŠ”", "ë‚˜ë§Œ ì•Œê³  ìˆëŠ”", "ë¶ë¹„ì§€ ì•ŠëŠ”", "í•œì í•œ"],
        "ìš°ì—°": ["ìš°ì—°íˆ", "ìš°ì—°í•œ", "ìš°ì—°íˆ ë°œê²¬í•œ", "ìš°ì—°íˆ ì•Œê²Œ ëœ", "ìš°ì—°íˆ ì°¾ì€", "ìš°ì—°íˆ ë°©ë¬¸í•œ", "ìš°ì—°íˆ ê°€ê²Œ ëœ"],
        "ë¡œì»¬": ["ë¡œì»¬", "í˜„ì§€ì¸", "ì£¼ë¯¼", "ë™ë„¤", "ë‹¨ê³¨", "local", "ê·¼ì²˜", "ì£¼ë³€"]
    }
    
    query_lower = query.lower()
    minor_types = []
    
    for minor_type, keywords in keyword_groups.items():
        if any(keyword in query_lower for keyword in keywords):
            minor_types.append(minor_type)
    
    return minor_types


# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_data(query_type: str) -> Tuple[List[Document], Any]:
    """ë°ì´í„° ë¡œë“œ í•¨ìˆ˜"""
    # í˜„ì¬ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì–»ê¸°
    current_dir = Path(__file__).resolve().parent.parent.parent  # RA6_vacAItion ë””ë ‰í† ë¦¬ê¹Œì§€
    
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    
    if query_type == "event":
        # ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        event_path = current_dir / "data/event_db/event_data.csv"
        event_vectorstore_path = current_dir / "data/event_db/vectorstore"
        
        if not event_path.exists():
            raise FileNotFoundError(f"ì´ë²¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {event_path}")
            
        # ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        event_df = pd.read_csv(event_path)
        event_df.columns = event_df.columns.str.strip()
        
        # ì´ë²¤íŠ¸ ë¬¸ì„œ ë³€í™˜
        event_docs = []
        for idx, row in event_df.iterrows():
            try:
                address_full = f"{row['location']} {row['address']} {row['address_detail']}"
                page_content = f"{row['title']}\nìœ„ì¹˜: {address_full}\nì‹œê°„: {row['time']}\në‚´ìš©: {row['content']}\në¶„ìœ„ê¸°: {row['atmosphere']}\nì¶”ì²œ ë™ë°˜ì: {row['companions']}"
                
                doc = Document(
                    page_content=page_content,
                    metadata={
                        "title": row['title'],
                        "url": "None",
                        "date": row['time'],
                        "location": row['location'],
                        "address": row['address'],
                        "address_detail": row['address_detail'],
                        "type": "event",
                        "tag": row['tag']
                    },
                )
                event_docs.append(doc)
            except Exception as e:
                print(f"ì´ë²¤íŠ¸ ë¬¸ì„œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                continue
        
        # ì´ë²¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            event_vectorstore = FAISS.load_local(str(event_vectorstore_path), embeddings, allow_dangerous_deserialization=True)
        except Exception:
            if event_docs:
                event_vectorstore = FAISS.from_documents(event_docs, embeddings)
                event_vectorstore.save_local(str(event_vectorstore_path))
            else:
                raise ValueError("ì´ë²¤íŠ¸ ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        return event_docs, event_vectorstore
        
    else:
        # ì¼ë°˜ ë°ì´í„° ë¡œë“œ
        db_path = current_dir / "data/db/documents.csv"
        vectorstore_path = current_dir / "data/db/vectorstore"
        
        if not db_path.exists():
            raise FileNotFoundError(f"ì¼ë°˜ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
            
        # ì¼ë°˜ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(db_path)
        
        # ì¼ë°˜ ë¬¸ì„œ ë³€í™˜
        docs = []
        for _, row in df.iterrows():
            try:
                content = row["content"]
                if isinstance(content, str):
                    try:
                        metadata_str = content.replace("('metadata', ", "").strip(")")
                        metadata_dict = eval(metadata_str)
                        line_number = metadata_dict.get("line_number", 0)

                        url = row["url"]
                        if isinstance(url, str) and "('page_content', '" in url:
                            content_parts = url.split("('page_content', '")[1].rsplit("')", 1)
                            if content_parts:
                                page_content = content_parts[0]

                                doc = Document(
                                    page_content=page_content,
                                    metadata={
                                        "title": "None",
                                        "url": url.split("\\t")[0] if "\\t" in url else url,
                                        "date": row["date"],
                                        "line_number": line_number,
                                        "type": "general"
                                    },
                                )
                                docs.append(doc)
                    except Exception:
                        continue
            except Exception:
                continue
                
        # ì¼ë°˜ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            vectorstore = FAISS.load_local(str(vectorstore_path), embeddings, allow_dangerous_deserialization=True)
        except Exception:
            if docs:
                vectorstore = FAISS.from_documents(docs, embeddings)
                vectorstore.save_local(str(vectorstore_path))
            else:
                raise ValueError("ì¼ë°˜ ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        return docs, vectorstore


# ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ ë³€í™˜ê¸°
def format_naver_results(places: List[Dict]) -> str:
    """ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not places:
        return "ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    result = "=== ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ===\n"
    for i, place in enumerate(places, 1):
        result += f"""
{i}. {place['title']}
   ğŸ“ ì£¼ì†Œ: {place['address']}
   ğŸ·ï¸ ë¶„ë¥˜: {place['category']}
   ğŸ” ë§í¬: {place.get('link', 'N/A')}
"""
    return result


# ë¬¸ì„œ í˜•ì‹ ë³€í™˜ê¸°
def format_documents(docs: List[Document]) -> str:
    """ë¬¸ì„œ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not docs:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
    formatted_docs = []
    for doc in docs:
        content = doc.page_content
        url = doc.metadata.get("url", "None")
        formatted_docs.append(f"{content}\nURL: {url}")
    return "\n\n".join(formatted_docs)


def initialize_graph():
    """LangGraph ì´ˆê¸°í™”"""
    global _graph_instance, graph_ready
    
    print("=== LangGraph ì´ˆê¸°í™” í•¨ìˆ˜ ì‹œì‘ ===")
    
    # ìƒíƒœ ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(GraphState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("query_analyzer", query_analyzer)
    workflow.add_node("hybrid_retriever", hybrid_retriever)
    workflow.add_node("naver_search", naver_search)
    workflow.add_node("response_generator", response_generator)
    
    # ì—£ì§€ ì—°ê²°
    workflow.set_entry_point("query_analyzer")
    workflow.add_edge("query_analyzer", "hybrid_retriever")
    workflow.add_edge("hybrid_retriever", "naver_search")
    workflow.add_edge("naver_search", "response_generator")
    workflow.add_edge("response_generator", END)
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    graph = workflow.compile()
    
    # ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
    _graph_instance = graph
    
    # í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (í™•ì¸ìš©)
    try:
        print("=== í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ë¹„ì–´ ìˆëŠ” ì§ˆë¬¸) ===")
        test_result = _graph_instance.invoke({"question": ""})
        print(f"=== í…ŒìŠ¤íŠ¸ í˜¸ì¶œ ì„±ê³µ, ê²°ê³¼ í‚¤: {test_result.keys()} ===")
    except Exception as e:
        print(f"=== í…ŒìŠ¤íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨: {e} ===")
    
    # ì¤€ë¹„ ì™„ë£Œ ì•Œë¦¼
    graph_ready.set()
    print("=== LangGraph ì´ˆê¸°í™” ì™„ë£Œ, graph_ready ì„¤ì •ë¨ ===")
    
    return graph


# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê·¸ë˜í”„ ì´ˆê¸°í™”
def initialize_graph_in_background():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê·¸ë˜í”„ ì´ˆê¸°í™”"""
    global _initialization_in_progress, _graph_instance
    
    # ì´ë¯¸ ì´ˆê¸°í™”ê°€ ì™„ë£Œëœ ê²½ìš°
    if _graph_instance is not None:
        print("=== LangGraph ì¸ìŠ¤í„´ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ===")
        return
        
    # ì´ë¯¸ ì´ˆê¸°í™”ê°€ ì§„í–‰ ì¤‘ì¸ ê²½ìš°
    if _initialization_in_progress:
        print("=== LangGraph ì´ˆê¸°í™”ê°€ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤ ===")
        return
    
    # ì´ˆê¸°í™” ì§„í–‰ ì¤‘ í”Œë˜ê·¸ ì„¤ì •
    _initialization_in_progress = True
    
    try:
        print("=== LangGraph ì´ˆê¸°í™” ì‹œì‘ ===")
        initialize_graph()
        print("=== LangGraph ì´ˆê¸°í™” ì™„ë£Œ - ì´ì œ ì‘ë‹µ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤ ===")
    except Exception as e:
        print(f"=== LangGraph ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} ===")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í”Œë˜ê·¸ ì´ˆê¸°í™”
        _initialization_in_progress = False


# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìë™ ì´ˆê¸°í™” ì½”ë“œëŠ” apps.pyì—ì„œ ê´€ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
# ì´ˆê¸°í™” ë¡œì§ì€ ChatbotConfig.ready()ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
# if _graph_instance is None and not _initialization_in_progress:
#     print("=== ì„œë²„ ì‹œì‘: LangGraph ì´ˆê¸°í™” ì‹œì‘ ===")
#     threading.Thread(target=initialize_graph_in_background, daemon=True).start() 